#!/usr/bin/env python
"""
ê°œë³„ í…ŒìŠ¤íŠ¸ cassetteì—ì„œ ê³µí†µ ticker ì´ˆê¸°í™” ìš”ì²­ì„ ì¶”ì¶œí•˜ì—¬
common cassetteìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ê³µí†µ cassette:
- etx_ticker_init.yaml: EtxTicker ì´ˆê¸°í™” (ETF/ETN/ELW ticker ëª©ë¡)
- stock_ticker_init.yaml: StockTicker ì´ˆê¸°í™” (KOSPI/KOSDAQ/KONEX ticker ëª©ë¡)

ì‹¤í–‰ í›„:
1. ê° í…ŒìŠ¤íŠ¸ cassetteì—ì„œ ê³µí†µ ìš”ì²­ ì œê±°
2. íŒŒì¼ í¬ê¸° ëŒ€í­ ê°ì†Œ (119MB â†’ ì˜ˆìƒ 20-30MB)
"""

import urllib.parse
from pathlib import Path

import yaml

PROJECT_ROOT = Path(__file__).parent.parent
CASSETTES_DIR = PROJECT_ROOT / "tests" / "cassettes"
COMMON_DIR = CASSETTES_DIR / "common"

# ë‚ ì§œ íŒŒë¼ë¯¸í„° í‚¤
DATE_KEYS = {
    "strtDd",
    "endDd",
    "trdDd",
    "fromdate",
    "todate",
    "startDt",
    "endDt",
    "stDt",
    "enDt",
    "date",
}

# BLD ì½”ë“œ ê·¸ë£¹ (ë‚ ì§œ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²½ìš°ë§Œ ê³µí†µí™”)
ETX_TICKER_BLDS = {
    "dbms/MDC/STAT/standard/MDCSTAT04601",  # ETF ticker
    "dbms/MDC/STAT/standard/MDCSTAT04801",  # ETN ticker
    "dbms/MDC/STAT/standard/MDCSTAT04301",  # ELW ticker
}

STOCK_TICKER_BLDS = {
    "dbms/MDC/STAT/standard/MDCSTAT01901",  # ì „ì²´ ticker (KOSPI/KOSDAQ/KONEX)
}

FINDER_BLDS = {
    "dbms/comm/finder/finder_stkisu",  # ì¢…ëª© ê¸°ë³¸ì •ë³´
    "dbms/comm/finder/finder_listdelisu",  # ìƒì¥íì§€ ì¢…ëª©
}

INDEX_KIND_BLDS = {
    "dbms/MDC/STAT/standard/MDCSTAT06701",  # ì§€ìˆ˜ êµ¬ì„±ì¢…ëª©
    "dbms/MDC/STAT/standard/MDCSTAT08501",  # ì—…ì¢…/ì„¹í„° êµ¬ì„±ì¢…ëª©
}


def normalize_body(body) -> str:
    """Bodyë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  URL decode."""
    if body is None:
        return ""
    if isinstance(body, dict):
        body = body.get("string", "")
    if isinstance(body, bytes):
        body = body.decode()
    if not isinstance(body, str):
        return ""
    return urllib.parse.unquote(body)


def extract_bld_from_body(body: str) -> str:
    """POST bodyì—ì„œ bld íŒŒë¼ë¯¸í„° ì¶”ì¶œ (URL-decoded)"""
    if not body:
        return ""
    for param in body.split("&"):
        if param.startswith("bld="):
            return param.split("=", 1)[1]
    return ""


def has_date_param(body: str) -> bool:
    """Body ì•ˆì— ë‚ ì§œ ê´€ë ¨ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ”ì§€ ê²€ì‚¬"""
    for param in body.split("&"):
        key = param.split("=", 1)[0]
        if key in DATE_KEYS:
            return True
    return False


def categorize_interactions(cassette_data: dict) -> dict:
    """Cassetteì˜ interactionsë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    categories = {
        "etx_ticker": [],
        "stock_ticker": [],
        "finder": [],
        "index_kind": [],
        "test_specific": [],
    }

    for interaction in cassette_data.get("interactions", []):
        request = interaction.get("request", {})
        body_str = normalize_body(request.get("body", {}))

        bld = extract_bld_from_body(body_str)
        if not bld:
            categories["test_specific"].append(interaction)
            continue

        # ë‚ ì§œ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ìš”ì²­ì€ ê³µí†µí™”í•˜ì§€ ì•ŠìŒ
        if has_date_param(body_str):
            categories["test_specific"].append(interaction)
            continue

        if bld in ETX_TICKER_BLDS:
            categories["etx_ticker"].append(interaction)
        elif bld in STOCK_TICKER_BLDS:
            categories["stock_ticker"].append(interaction)
        elif bld in FINDER_BLDS:
            categories["finder"].append(interaction)
        elif bld in INDEX_KIND_BLDS:
            categories["index_kind"].append(interaction)
        else:
            categories["test_specific"].append(interaction)

    return categories


def merge_common_interactions(existing: list, new: list) -> list:
    """ê³µí†µ interaction ë³‘í•© (ì¤‘ë³µ ì œê±°)"""
    # ìš”ì²­ ì‹œê·¸ë‹ˆì²˜ë¡œ ì¤‘ë³µ ì²´í¬
    seen = set()
    merged = []

    for interaction in existing + new:
        request = interaction.get("request", {})
        uri = request.get("uri", {})
        body_str = normalize_body(request.get("body", {}))

        bld = extract_bld_from_body(body_str)
        signature = (uri, bld)

        if signature not in seen:
            seen.add(signature)
            merged.append(interaction)

    return merged


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="ë‚ ì§œ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê³µí†µ í˜¸ì¶œì„ cassetteì—ì„œ ì¶”ì¶œí•´ ê³µìœ  cassetteìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œë¡œ íŒŒì¼ì„ ìˆ˜ì •í•˜ì§€ ì•Šê³  ê²°ê³¼ë§Œ ì¶œë ¥",
    )
    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ“¦ ê³µí†µ Cassette ì¶”ì¶œ ì‹œì‘")
    print("=" * 70)

    # ê³µí†µ cassette ì´ˆê¸°í™”
    common_cassettes = {
        "etx_ticker": {"version": 1, "interactions": []},
        "stock_ticker": {"version": 1, "interactions": []},
        "finder": {"version": 1, "interactions": []},
        "index_kind": {"version": 1, "interactions": []},
    }

    cassette_files = list(CASSETTES_DIR.glob("Test*.yaml"))
    print(f"\nğŸ“‹ {len(cassette_files)}ê°œ cassette íŒŒì¼ ë¶„ì„ ì¤‘...\n")

    modified_count = 0
    total_removed = 0

    for cassette_path in cassette_files:
        try:
            with open(cassette_path, encoding="utf-8") as f:
                data = yaml.safe_load(f)

            if not data or "interactions" not in data:
                continue

            original_count = len(data["interactions"])

            # Interaction ë¶„ë¥˜
            categories = categorize_interactions(data)

            # ê³µí†µ cassetteì— ì¶”ê°€
            for key in ("etx_ticker", "stock_ticker", "finder", "index_kind"):
                common_cassettes[key]["interactions"] = merge_common_interactions(
                    common_cassettes[key]["interactions"],
                    categories[key],
                )

            # í…ŒìŠ¤íŠ¸ ê³ ìœ  ìš”ì²­ë§Œ ë‚¨ê¹€
            removed = original_count - len(categories["test_specific"])
            if removed > 0:
                if not args.dry_run:
                    data["interactions"] = categories["test_specific"]
                    with open(cassette_path, "w", encoding="utf-8") as f:
                        yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

                print(
                    f"  {'[DRY RUN] ' if args.dry_run else ''}âœ‚ï¸  {cassette_path.name}"
                )
                print(
                    f"     {original_count} â†’ {len(categories['test_specific'])} interactions (-{removed})"
                )
                modified_count += 1
                total_removed += removed

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {cassette_path.name} - {e}")

    # ê³µí†µ cassette ì €ì¥
    print("\nğŸ’¾ ê³µí†µ Cassette ì €ì¥ ì¤‘...")

    if not args.dry_run:
        COMMON_DIR.mkdir(exist_ok=True)

    for name, data in common_cassettes.items():
        if data["interactions"]:
            cassette_path = COMMON_DIR / f"{name}_init.yaml"

            if not args.dry_run:
                with open(cassette_path, "w", encoding="utf-8") as f:
                    yaml.dump(data, f, default_flow_style=False, allow_unicode=True)

            print(
                f"  {'[DRY RUN] ' if args.dry_run else ''}âœ… {cassette_path.name} ({len(data['interactions'])} interactions)"
            )

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print(f"âœ¨ {modified_count}ê°œ íŒŒì¼ ìˆ˜ì •")
    print(f"ğŸ—‘ï¸  ì´ {total_removed}ê°œ ì¤‘ë³µ interaction ì œê±°")

    if args.dry_run:
        print("\nâš ï¸  DRY RUN ëª¨ë“œ: ì‹¤ì œ íŒŒì¼ì´ ìˆ˜ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì‹¤ì œë¡œ ì ìš©í•˜ë ¤ë©´ --dry-run ì—†ì´ ì‹¤í–‰í•˜ì„¸ìš”.")
    else:
        print(f"\nğŸ“‚ ê³µí†µ cassette ìœ„ì¹˜: {COMMON_DIR}")
        print("   ê° í…ŒìŠ¤íŠ¸ëŠ” ì´ì œ ê³µí†µ cassetteì„ ìë™ìœ¼ë¡œ ì°¸ì¡°í•©ë‹ˆë‹¤.")

    print("=" * 70)


if __name__ == "__main__":
    main()
